% ==============================================================================
% BIBLIOGRAPHY FILE FOR FINDINGS SECTION
% ==============================================================================
% Save this as references.bib in your Overleaf project

@article{tatqa,
  title={TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance},
  author={Zhu, Fengbin and Lei, Wenqiang and Huang, Youcheng and Wang, Chao and Zhang, Shuo and Lv, Jiancheng and Feng, Fuli and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2105.07624},
  year={2021},
  url={https://arxiv.org/abs/2105.07624},
  note={Dataset: \url{https://github.com/NExTplusplus/TAT-QA}}
}

@inproceedings{finqa,
  title={FinQA: A dataset of numerical reasoning over financial data},
  author={Chen, Zhiyu and Chen, Wenhu and Smiley, Charese and Shah, Sameena and Borova, Iana and Langdon, Dylan and Moussa, Reema and Beane, Matt and Huang, Ting-Hao and Routledge, Bryan and Wang, William Yang},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={3697--3711},
  year={2021},
  organization={Association for Computational Linguistics},
  url={https://aclanthology.org/2021.emnlp-main.300/},
  note={Dataset: \url{https://github.com/czyssrs/FinQA}}
}

@article{ragas,
  title={RAGAS: Automated evaluation of retrieval augmented generation},
  author={Es, Shahul and James, Jithin and Espinosa-Anke, Luis and Schockaert, Steven},
  journal={arXiv preprint arXiv:2309.15217},
  year={2023},
  url={https://arxiv.org/abs/2309.15217},
  note={Framework: \url{https://github.com/explodinggradients/ragas}}
}

@inproceedings{es2023ragas,
  title={Evaluating retrieval augmented generation: A survey},
  author={Es, Shahul and James, Jithin and Espinosa-Anke, Luis and Schockaert, Steven},
  booktitle={arXiv preprint arXiv:2405.07437},
  year={2024},
  url={https://arxiv.org/abs/2405.07437}
}

@article{chen2023benchmarking,
  title={Benchmarking large language models in retrieval-augmented generation},
  author={Chen, Jiawei and Lin, Hongyu and Han, Xianpei and Sun, Le},
  journal={arXiv preprint arXiv:2309.01431},
  year={2023},
  url={https://arxiv.org/abs/2309.01431}
}

@article{lfm2,
  title={Liquid Foundation Models: Our First Series of Generative AI Models},
  author={LiquidAI},
  year={2024},
  url={https://www.liquid.ai/liquid-foundation-models},
  note={Model: LFM2-1.2B-RAG}
}

@article{sentencetransformers,
  title={Sentence-BERT: Sentence embeddings using Siamese BERT-networks},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019},
  url={https://arxiv.org/abs/1908.10084},
  note={Model: all-MiniLM-L6-v2}
}

@inproceedings{chromadb,
  title={Chroma: Open-source embedding database},
  author={Chroma Team},
  year={2023},
  url={https://www.trychroma.com/},
  note={Vector database for AI applications}
}

@article{bertscore,
  title={BERTScore: Evaluating text generation with BERT},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019},
  url={https://arxiv.org/abs/1904.09675}
}

@inproceedings{rouge,
  title={ROUGE: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004},
  url={https://aclanthology.org/W04-1013/}
}

@article{lewis2020rag,
  title={Retrieval-augmented generation for knowledge-intensive NLP tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020},
  url={https://arxiv.org/abs/2005.11401}
}

@article{guu2020realm,
  title={REALM: Retrieval-augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
  journal={arXiv preprint arXiv:2002.08909},
  year={2020},
  url={https://arxiv.org/abs/2002.08909}
}

@article{borgeaud2022improving,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2112.04426},
  year={2022},
  url={https://arxiv.org/abs/2112.04426}
}

@inproceedings{asai2023selfrag,
  title={Self-RAG: Learning to retrieve, generate, and critique through self-reflection},
  author={Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
  booktitle={arXiv preprint arXiv:2310.11511},
  year={2023},
  url={https://arxiv.org/abs/2310.11511}
}

% Additional relevant papers you might want to cite

@article{zhao2023retrievalaugmented,
  title={Retrieval-augmented generation for AI-generated content: A survey},
  author={Zhao, Penghao and Zhang, Hailin and Yu, Qinhan and Wang, Zhengren and Geng, Yunteng and Fu, Fangcheng and Yang, Ling and Zhang, Wentao and Cui, Bin},
  journal={arXiv preprint arXiv:2402.19473},
  year={2024},
  url={https://arxiv.org/abs/2402.19473}
}

@article{gao2023retrievalaugmented,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023},
  url={https://arxiv.org/abs/2312.10997}
}
